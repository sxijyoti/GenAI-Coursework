{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOM64km+wHJZkt9u80i8O+e"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Unit 2 - Part 4a: Embeddings & Vector Space"
      ],
      "metadata": {
        "id": "MQblwXml5rzN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6E5cMbr5R-1",
        "outputId": "d65b36ab-faa9-4911-9d29-54b3c38e8c16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "%pip install python-dotenv --upgrade --quiet langchain langchain-huggingface sentence-transformers langchain-community\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# Using a FREE, open-source model from Hugging Face\n",
        "# 'all-MiniLM-L6-v2' is small, fast, and very good for English.\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector = embeddings.embed_query(\"Orange\")\n",
        "\n",
        "print(f\"Dimensionality: {len(vector)}\")\n",
        "print(f\"First 5 numbers: {vector[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt0EGIlP5YfJ",
        "outputId": "11db0cac-63d4-4a0b-f96b-1b993e14bf34"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensionality: 384\n",
            "First 5 numbers: [-0.006138487718999386, 0.03101177327334881, 0.06479360908269882, 0.01094149798154831, 0.005267191678285599]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "vec_cat = embeddings.embed_query(\"Cat\")\n",
        "vec_dog = embeddings.embed_query(\"Dog\")\n",
        "vec_car = embeddings.embed_query(\"Car\")\n",
        "\n",
        "print(f\"Cat vs Dog: {cosine_similarity(vec_cat, vec_dog):.4f}\")\n",
        "print(f\"Cat vs Car: {cosine_similarity(vec_cat, vec_car):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Od--td3k5cRP",
        "outputId": "f4123244-7a59-4260-8cad-9950a9bfe193"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cat vs Dog: 0.6606\n",
            "Cat vs Car: 0.4633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unit 2 - Part 4b: Naive RAG Pipeline"
      ],
      "metadata": {
        "id": "7fdn2LQB5mUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "%pip install python-dotenv --upgrade --quiet faiss-cpu langchain-huggingface sentence-transformers langchain-community langchain_google_genai\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "import getpass\n",
        "from os import environ\n",
        "\n",
        "# if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "#   os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "# Using the same free model as Part 4a\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9PFSQ6Y5hyX",
        "outputId": "3999395e-92cd-4c73-b2f7-c157647a54cf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/66.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "docs = [\n",
        "    Document(page_content=\"Saijyoti's favorite food is Ramen and Tiramisu.\"),\n",
        "    Document(page_content=\"The secret password to the lab is 'MEOW'.\"),\n",
        "    Document(page_content=\"LangChain is a framework for developing applications powered by language models.\"),\n",
        "]"
      ],
      "metadata": {
        "id": "AZJh4lf_52ny"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "IsiEvdMT55xE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "template = \"\"\"\n",
        "Answer based ONLY on the context below:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "result = chain.invoke(\"What is the secret password?\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3WUj_vn58x9",
        "outputId": "e46d1440-0a2c-4aca-cf19-8ac61d41cd50"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The secret password to the lab is 'MEOW'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unit 2 - Part 4c: Deep Dive into Indexing Algorithms"
      ],
      "metadata": {
        "id": "LkwBUfdR6CEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Mock Data: 10,000 vectors of size 128\n",
        "d = 128\n",
        "nb = 10000\n",
        "xb = np.random.random((nb, d)).astype('float32')"
      ],
      "metadata": {
        "id": "1CwONaTd5_ms"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = faiss.IndexFlatL2(d)\n",
        "index.add(xb)\n",
        "print(f\"Flat Index contains {index.ntotal} vectors\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7RyAYUk6GAV",
        "outputId": "8acb1edf-9c39-44bd-b619-611e69a92e56"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flat Index contains 10000 vectors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlist = 100 # How many 'zip codes' (clusters) we want\n",
        "quantizer = faiss.IndexFlatL2(d) # The calculator for distance\n",
        "index_ivf = faiss.IndexIVFFlat(quantizer, d, nlist)\n",
        "\n",
        "# We MUST train it first so it learns where the clusters are\n",
        "index_ivf.train(xb)\n",
        "index_ivf.add(xb)"
      ],
      "metadata": {
        "id": "FwTDAyHr6IWI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "M = 16 # Number of connections per node (The 'Hub' factor)\n",
        "index_hnsw = faiss.IndexHNSWFlat(d, M)\n",
        "index_hnsw.add(xb)"
      ],
      "metadata": {
        "id": "krP6wvPK6K8u"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = 8 # Split vector into 8 sub-vectors\n",
        "index_pq = faiss.IndexPQ(d, m, 8)\n",
        "index_pq.train(xb)\n",
        "index_pq.add(xb)\n",
        "print(\"PQ Compression complete. RAM usage minimized.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LiC4u-_6Nfu",
        "outputId": "64f4e44f-1063-49f5-ac0b-d0b491be328a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PQ Compression complete. RAM usage minimized.\n"
          ]
        }
      ]
    }
  ]
}