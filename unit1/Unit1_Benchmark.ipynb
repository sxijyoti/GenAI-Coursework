{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": []}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}}, "cells": [{"cell_type": "code", "source": ["# SRN: PES2UG23CS512"], "metadata": {"id": "kDUEtMiiZN_6"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "execution_count": 1, "metadata": {"id": "7Jg1lDsB40_t", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "f3a0a9cc-9ea6-4483-9dea-aa5ebbee0df7"}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n", "Collecting pipeline\n", "  Downloading pipeline-0.1.0-py3-none-any.whl.metadata (483 bytes)\n", "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n", "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n", "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n", "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n", "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n", "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n", "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n", "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n", "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n", "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n", "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n", "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n", "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n", "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n", "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n", "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n", "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n", "Downloading pipeline-0.1.0-py3-none-any.whl (2.6 kB)\n", "Installing collected packages: pipeline\n", "Successfully installed pipeline-0.1.0\n"]}], "source": ["!pip install transformers pipeline"]}, {"cell_type": "code", "source": ["from transformers import pipeline, set_seed, GPT2Tokenizer"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "bA698aU11lcB", "outputId": "1d3fe789-5518-4aa4-f55a-3aa72c736667"}, "execution_count": 2, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"]}]}, {"cell_type": "code", "source": ["set_seed(42)"], "metadata": {"id": "TS6lHYOc1ykY"}, "execution_count": 3, "outputs": []}, {"cell_type": "markdown", "source": ["# Experiment 1: Text Generation"], "metadata": {"id": "U1ys0Rll2IXf"}}, {"cell_type": "code", "source": ["prompt = \"The future of Artificial Intelligence is\""], "metadata": {"id": "HITBgmY73H4e"}, "execution_count": 4, "outputs": []}, {"cell_type": "markdown", "source": ["## MODEL 1: BERT\n"], "metadata": {"id": "yX_hx-cS2NMw"}}, {"cell_type": "code", "source": ["bert = pipeline('text-generation', model='distilbert-base-uncased')\n", "\n", "# output for bert to generate text\n", "bert_output = bert(prompt, max_length=50, num_return_sequences=1)\n", "print(bert_output[0]['generated_text'])"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 418}, "id": "uTMsUhha1-lX", "outputId": "688b1e54-c57f-40cf-eca2-9042884751eb"}, "execution_count": 6, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["Device set to use cpu\n"]}, {"output_type": "error", "ename": "AttributeError", "evalue": "'TextGenerationPipeline' object has no attribute 'assistant_model'", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)", "\u001b[0;32m/tmp/ipython-input-34628690.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text-generation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'distilbert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# output for bert to generate text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbert_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'generated_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processor\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpipeline_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         self.check_model_type(\n\u001b[1;32m    123\u001b[0m             \u001b[0mTF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tf\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mMODEL_FOR_CAUSAL_LM_MAPPING_NAMES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, device, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"num_workers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_postprocess_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;31m# In processor only mode, we can get the modality processors from the processor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_sanitize_parameters\u001b[0;34m(self, return_full_text, return_tensors, return_text, return_type, clean_up_tokenization_spaces, prefix, handle_long_generation, stop_sequence, truncation, max_length, continue_final_message, skip_special_tokens, tokenizer_encode_kwargs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eos_token_id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop_sequence_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mforward_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massistant_model\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0mforward_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"assistant_model\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massistant_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massistant_tokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mAttributeError\u001b[0m: 'TextGenerationPipeline' object has no attribute 'assistant_model'"]}]}, {"cell_type": "markdown", "source": ["## MODEL 2: RoBERTa\n"], "metadata": {"id": "syh5bFup2vYz"}}, {"cell_type": "code", "source": ["roberta_generator = pipeline('text-generation', model='roberta-base')\n", "\n", "# output for roberta to generate text\n", "roberta_output = roberta_generator(prompt, max_length=50, num_return_sequences=1)\n", "print(roberta_output[0]['generated_text'])"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 318, "referenced_widgets": ["4cee2b38f5564401b46b40678313c3ec", "b817c1a9142f47a3a50625a613d2666f", "32efdc93937c41e2b40ac470c6cedf1c", "a193025c46534817a6d37c9d758d25c8", "e4e05f72753247f8bcd75e45af0afd1f", "4203aa8640a8416e85db41e89034e5df", "33015ac1f6884bb5af423035dacee73e", "0738655e972246159cf731af32bd5699", "81ce1c47ccde4e22b6d4d81590fbf5e5", "35a761cdf0744350a80ad5e3f8a5bbf4", "c244c0b9c0f74ae9911fedc10241e2a9", "2f8c58b71e524740a004fda559be8cc0", "35248bb689ed4a1a891938a6e46cdf60", "80e59ca46e9c488f9c0e538bcee81b1f", "76885cce29b04ff3b51d5afbaf99ece3", "f038da9a9d044efdb3ca0fe6b1ba7ab1", "f177f095a054435cab6d20b489d46221", "cf019376884d400884203ed4b6a6591f", "f128e29854ac4270b85ecf840edcc985", "34f2444db98644e7b8cc3d2130dc2eff", "e1267c573bc74a47aa49f863cd0aec8b", "04e7f51e77d54c24a1f1e367acaed9b5", "c31766eaaa054ec7baacbc47c671b832", "9dbbdd2cc0734ecc9fca0c4f5b80393d", "2bb47a4bef0c48c096099207af5b8ede", "8144ed524cc94eb8bc75355bdc7254fc", "ecff8e2e5f3048158c2c1d10ff713404", "8efcf02ccc6446e68d58f53ee8101e2a", "de5269007af64c248c9cacef65c27e86", "c84e4379272a4674817cffaf87a0996b", "3b5f500b87ec42c7806127d7fa568bf4", "e3bc7e4c814c45498008418dd768931b", "bec9f28984e04d98be852c4f99439dc4", "0ffb4cc819ce41f79727013e72bfe401", "c5f7dd1b078449cbb38c961236032386", "9c23a93d7b264692966a0ddc77e99f0b", "4627e3f65e3e46b3b0e2a43e5b3ae764", "94ba8cc0864a436a8237c6ad2b518d6b", "ac4cfc75d26a44ce998d70f371682a54", "c3f4740915204604b5d943c259d0eca9", "127709a38b8c4f8d9a98587948df8adc", "95f1423dcddf4cffb93d3f9a251a0c4e", "13e55837dba2403197976ca16192e408", "c571f1b564814fe99130b834263c2aa8", "cbb3addd0ec84769bd84fa1ed22c740e", "d56e82c996f145558f83f8c6ac517d42", "5e2d9ee8266645d7a69b27f858b5e848", "3c96186652054e0fa6f80158f527f97d", "25e7822ca49f46888751741add77d1c7", "0a3fc5450be34a95991e93d3cbaec8e6", "5e4a44cd70f14c4ab175f7329a42b71f", "2f5779866c684616b9ca388d0a8ebba2", "ec604ded0f50419cae7ad3d6290af37a", "58b1c454639a44a2995ad9dcfde4d91e", "6182892a976c4cfa88988a3c9c0d614a", "b1d0f2d9c80045e5a3e573a753f31fa8", "56194335c80849648938e98018dc3cee", "a610b07e1845452a87d5aa694aa06031", "a7b0c7f1d6564c0394c5b4d1412d033c", "ba589b89550a4a0da33f7ade32b85273", "a50da66a58464c3cbe9a7900a8c37d92", "511bc9ba1bbf48f6bd6c5f656c38f050", "ff50f34933874dd6932ca3bdcf88f28e", "52255188934246ea9a608ae96b58c246", "86c51c575c994cea8bc7e2a40304cae1", "f7deb2fa092c46ef94082d8cab0689e9"]}, "id": "ewK13yxL23Wr", "outputId": "eda87c1c-e505-4412-b86f-7771f8305a6a"}, "execution_count": 7, "outputs": [{"output_type": "display_data", "data": {"text/plain": ["config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "4cee2b38f5564401b46b40678313c3ec"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2f8c58b71e524740a004fda559be8cc0"}}, "metadata": {}}, {"output_type": "stream", "name": "stderr", "text": ["If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"]}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c31766eaaa054ec7baacbc47c671b832"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0ffb4cc819ce41f79727013e72bfe401"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "cbb3addd0ec84769bd84fa1ed22c740e"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b1d0f2d9c80045e5a3e573a753f31fa8"}}, "metadata": {}}, {"output_type": "stream", "name": "stderr", "text": ["Device set to use cpu\n", "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n", "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]}, {"output_type": "stream", "name": "stdout", "text": ["The future of Artificial Intelligence is\n"]}]}, {"cell_type": "markdown", "source": ["## MODEL 3: BART\n"], "metadata": {"id": "YlZSO15M230w"}}, {"cell_type": "code", "source": ["bart = pipeline('text-generation', model='facebook/bart-base')\n", "\n", "# output for bart to generate text\n", "bart_output = bart(prompt, max_length=50, num_return_sequences=1)\n", "print(bart_output[0]['generated_text'])"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 304, "referenced_widgets": ["9fe3652f076c4be99d530483db908a4d", "7157d6aed1c2422788851777d6bf2440", "be55493c321246b8946b0e351bc54904", "693767875a514e07bb2ce72b9b904be9", "07265d6618cd4d60ba65b50f53a2bb99", "5a8c6fc0fbda4c8dac879eda4fe5a3a8", "0ec4d79abb794022aefc66f3566cf97d", "e502f00c9504494390738719e60c939f", "d7976641ec4845ce9bdd451c9c3ea648", "3921e7a141574309a9ff8496e3d81173", "502b4a1d95684603bd40828e8acbe71c", "dd7f5a5030fe4ab18cabf9fa1b711dea", "28cc27e1c6ef4ccd9241bce51bed8f49", "11b59a2200894b4a8ad4883881d81d91", "adb2c3bf42d04f47953461854afb9d58", "d49ef77730b74a42995ea3f0ceea3542", "dbe04b862605478caa7862df7da85362", "1963ee6d01e84c30b914d418093633ec", "1ad68d4b66244de28bab322de3d7acbc", "a54fcac540ce4a7e8bd15c4129ead3be", "0c837e818a3b466ea175766a43149012", "9ee6ceaa8a4245afb58a73cbf25e9de7", "34981232a16a476e99826c0f97942622", "ed8291d2756d4215915762a10353e11c", "bbf59781d8394d88a80ea50a2e536e1b", "4a13bc91aa104ecfadeace81a087c228", "32962b5acebe4c17b3036656614781ce", "fa66ad5b8a2848238fef236132a7d570", "84e92b2d7c2b4491965e9b5de7342724", "3684710d1acf4d20aa3840ebca0aa2d9", "a4bcb3ddf54c41df9878b2e06d2f7d6c", "947e153c5f9e45d09286bb1e6f5520be", "ed368c4d96ca49319b8c2d77a6448fd3", "975dc202414e45dab8706a0296c87731", "1c19e4b951a14615a9eaa1f8121c7854", "e60f3b4fc68c41a3b3babb751fe85d81", "63e151a16d1e4d7baf50975617a3139e", "f4ccc7b001714df6a1423b21ebcd56cd", "adc9f7a63ca644f5a24288a0058fdd8c", "d2093aa8c69742ddb78cd717de079a76", "9689e09dbfb948b9a1c33338ae51b734", "ad3fc93607f248378c7ffb31f88d0a85", "2e1c8fbc15c248f9ae919bcb3eef4d2b", "e2ecb4298a884fa9a0a2add530cb6cac", "89282173dffa4d769aa65ca4d04612bf", "076eaa07762f4c0e93b66d17a36a6172", "69f485ef1eda4d64b36771aad6613030", "f2209a2ebd8d44f2b2225070c0f7c91c", "dfcbd370439049e38ed597cb27564a0a", "0815bf5756c0458dbb9f7f308c1c4f1d", "3664161e574b4ff2941a68ee87900677", "f5480c6fec084231a0002b35b6918aee", "84fc55ee3ac448ccbd0b7d26e829dc5c", "24d61d6d0ecb4803abc6e1619d817a77", "6b4be4d79df044e4b4fdcef7164c2599"]}, "id": "478uwVTp3Gsx", "outputId": "9a52d81e-67cd-414b-a85f-0b10bea67ebd"}, "execution_count": 8, "outputs": [{"output_type": "display_data", "data": {"text/plain": ["config.json: 0.00B [00:00, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "9fe3652f076c4be99d530483db908a4d"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "dd7f5a5030fe4ab18cabf9fa1b711dea"}}, "metadata": {}}, {"output_type": "stream", "name": "stderr", "text": ["Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n", "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}, {"output_type": "display_data", "data": {"text/plain": ["vocab.json: 0.00B [00:00, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "34981232a16a476e99826c0f97942622"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["merges.txt: 0.00B [00:00, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "975dc202414e45dab8706a0296c87731"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer.json: 0.00B [00:00, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "89282173dffa4d769aa65ca4d04612bf"}}, "metadata": {}}, {"output_type": "stream", "name": "stderr", "text": ["Device set to use cpu\n", "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n", "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]}, {"output_type": "stream", "name": "stdout", "text": ["The future of Artificial Intelligence is Lands sorry sorry sorryya sorry sorry insiders wax802 sorry sorry802802802 attain sorry sorry\ufffd sorry sorry swinging sorry sorryillery sorry sorryixir sorry judge judge802 attain DOJ sorry sorry obtaining sorry sorry Pharmac judge sorry802 sorry attainouxievers sorry sorryrather sorry sorry Crush802 sorry802 comedy attain sorryjan sorry comedy INF judge judge sorry sorry retained judge sorryuba sorry sorry dads sorry Buckingham802802Lathered sorryintendo wax attain comedy judge obtainingazohered sorry sorry judge802 judge OFFIC802Lat DOJ judge comedy802 ((hered Sloan sorry obtaining perceive Sloan Sloan controlling sorryLat waxratherheredhered sorryhered judge Sloan ((802 comedy sorry judgerather sorry obtaining judge802rather Cthascar comedy sorry (( sorry comedy wax sorry comedypayment802rather\ufffd (( (( attain judge Sloan judge comedy judgeratherrather Gym (( (( judge sniffImpro comedy SloanratherAppearance (( comedy comedy comedy indicative Sloan comedy comedyhered IMFrather prominentlyratherpayment obtainingratherpaymentrather Lunratherratherrather obtainingratherrather perceiverather judge 344 IMFratherrather comedyImpro (( prominently judgerather 344 344 comedy IMFrather indicative 344 comedyratherrather overlooking obtaining802rather 344 comedypayment judgeImpro comedy\ufffd\ufffdpaymentover comedyheredrather CthImpro\ufffd\ufffd slip comedy802 firmratherratherIranImpro comedy commands 344 344 prominently802802 comedy algorithratherpayment 344payment\n"]}]}, {"cell_type": "markdown", "source": ["# Experiment 2: Masked Language Modeling (Missing Word)"], "metadata": {"id": "HFJpdxVa3n10"}}, {"cell_type": "code", "source": ["prompt2 = \"The goal of Generative AI is to [MASK] new content\""], "metadata": {"id": "wFdDHiFP32Hb"}, "execution_count": 10, "outputs": []}, {"cell_type": "markdown", "source": ["## MODEL 1: BERT\n"], "metadata": {"id": "7l1w7vGY3wer"}}, {"cell_type": "code", "source": ["bert = pipeline('fill-mask', model='distilbert-base-uncased')\n", "\n", "# generate output based on masked word for bert\n", "bert_output = bert(prompt2, top_k = 1)\n", "print(bert_output[0]['token_str'])"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "aLg5J8Yn31c5", "outputId": "723697bc-e88f-42d4-b5c4-09824c7f4fcc"}, "execution_count": 11, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["Device set to use cpu\n"]}, {"output_type": "stream", "name": "stdout", "text": ["generate\n"]}]}, {"cell_type": "markdown", "source": ["## MODEL 2: RoBERTa\n"], "metadata": {"id": "-JPk0q1f308m"}}, {"cell_type": "code", "source": ["prompt2_t2 = \"The goal of Generative AI is to <mask> new content\" # some models use different mask\n", "roberta = pipeline('fill-mask', model='roberta-base')\n", "\n", "# generate output based on masked word for roberta\n", "roberta_output = roberta(prompt2_t2, top_k = 1)\n", "print(roberta_output[0]['token_str'])"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "pt9Np05e36tn", "outputId": "1cd5dc79-7212-4d05-c12e-30a4f59439b0"}, "execution_count": 12, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["Device set to use cpu\n"]}, {"output_type": "stream", "name": "stdout", "text": [" create\n"]}]}, {"cell_type": "markdown", "source": ["## MODEL 3: BART"], "metadata": {"id": "U6TiQLFo37CM"}}, {"cell_type": "code", "source": ["bart = pipeline('fill-mask', model='facebook/bart-base')\n", "\n", "# generate output based on masked word for bart\n", "bart_output = bart(prompt2_t2, top_k = 1)\n", "print(bart_output[0]['token_str'])"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "fyIKoAPa3-aU", "outputId": "3fd38f63-c366-462c-8d42-3991cfc63820"}, "execution_count": 13, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["Device set to use cpu\n"]}, {"output_type": "stream", "name": "stdout", "text": [" create\n"]}]}, {"cell_type": "markdown", "source": ["# Experiment 3: Question Answering"], "metadata": {"id": "hRZ_OJ_44Al6"}}, {"cell_type": "code", "source": ["promptcontext = \"Generative AI poses significant risks such as hallucinations, bias and deepfakes.\"\n", "qs = \"What are the risks?\""], "metadata": {"id": "sVuNIcfQ4EVO"}, "execution_count": 14, "outputs": []}, {"cell_type": "markdown", "source": ["## MODEL 1: BERT"], "metadata": {"id": "jMLyR8Fj4HNW"}}, {"cell_type": "code", "source": ["qa_bert = pipeline('question-answering', model = 'distilbert-base-uncased')\n", "bert_output1 = (qa_bert(question = qs, context = promptcontext))\n", "print(bert_output1['answer'])"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "hftzJB4g4J9u", "outputId": "8e4d2429-c774-4771-ee03-a95cda6d507d"}, "execution_count": 17, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n", "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n", "Device set to use cpu\n"]}, {"output_type": "stream", "name": "stdout", "text": ["hallucinations, bias and deepfakes.\n"]}]}, {"cell_type": "markdown", "source": ["## MODEL 2: RoBERTa"], "metadata": {"id": "8x0OEW7K4L3y"}}, {"cell_type": "code", "source": ["qa_roberta = pipeline('question-answering', model=('roberta-base'))\n", "roberta_output = (qa_roberta(question = qs, context = promptcontext))\n", "print(roberta_output['answer'])"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "BJBqRsQt4M7v", "outputId": "1bcf4e1b-fad5-43eb-cd34-4e825f4d131d"}, "execution_count": 21, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n", "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n", "Device set to use cpu\n"]}, {"output_type": "stream", "name": "stdout", "text": ["Generative AI poses significant risks\n"]}]}, {"cell_type": "markdown", "source": ["## MODEL 3: BART"], "metadata": {"id": "LyoLvwcz4Nf8"}}, {"cell_type": "code", "source": ["qa_bart = pipeline('question-answering', model='facebook/bart-base')\n", "\n", "# Generate text\n", "bart_output1 = (qa_bart(question = qs, context = promptcontext))\n", "print(bart_output1['answer'])"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "MOL7WbDs4RO4", "outputId": "79d35376-e995-48cd-d244-22a0fcf9d8db"}, "execution_count": 23, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n", "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n", "Device set to use cpu\n"]}, {"output_type": "stream", "name": "stdout", "text": ["poses significant risks such as hallucinations, bias and deepfakes\n"]}]}, {"cell_type": "markdown", "source": ["# DELIVERABLE: OBSERVATION TABLE"], "metadata": {"id": "ZbMNhGYv5_88"}}, {"cell_type": "markdown", "source": ["\n", "\n", "| Task | Model | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n", "| :--- | :--- | :--- | :--- | :--- |\n", "| **Generation** | BERT | Failure | Threw an error. | BERT is an Encoder; it isn't trained to predict the next word. |\n", "| | RoBERTa | Failure | Returned the initial prompt as it is | RoBERTa is an encoder-only model and thus can't predict the next word |\n", "| | BART | Failure | Generated gibberish text as output| BART is an encoder-decoder and thus can generate text, but no fine-tuning can lead to irrelavant output|\n", "| **Fill-Mask** | BERT | Success | Predicted 'generate'.| BERT is trained on Masked Language Modeling (MLM). |\n", "| | RoBERTa | Success |Predicted 'create' |RoBERTa is an optimized variant of BERT trained extensively on MLM leading to stronger masked token predictions |\n", "| | BART | Success |Predicted 'create' |BART is trained with denoising objectives, allowing it to reconstruct masked tokens effectively. |\n", "| **QA** | BERT |Success |Returned \"hallucinations, bias and deepfakes.\" | BERT uses bidirectional self attention enabling strong contextual understanding|\n", "| | RoBERTa |Success |Returned \"Generative AI poses significant risks\" | Has a similar architecture as BERT with added fine-tuning |\n", "| | BART |Success | Returned \"poses significant risks such as hallucinations, bias and deepfakes\" | BART uses encoder-decoder architecture which supports both understanding and generative-style QA|\n"], "metadata": {"id": "lI0YIt825WMb"}}]}