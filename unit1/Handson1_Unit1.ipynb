{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": []}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}}, "cells": [{"cell_type": "code", "execution_count": 1, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "4MVIjKPrA4U_", "outputId": "8696c138-92f1-4fd3-cb5f-943246283793"}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n", "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n", "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n", "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n", "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n", "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n", "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n", "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n", "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n", "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n", "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n", "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n", "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n", "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.12.19)\n", "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n", "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n", "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n", "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n", "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n", "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n", "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n", "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n", "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.3.0)\n", "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n", "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n", "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n", "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n", "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n", "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n", "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n", "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n", "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n", "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n", "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n", "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n", "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n", "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n", "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n", "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n", "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n", "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n", "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n", "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n", "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n", "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n", "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n", "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n", "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n", "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n", "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n", "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.5)\n", "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n", "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n", "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n", "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"]}], "source": ["!pip install torch tensorflow transformers nltk"]}, {"cell_type": "code", "source": ["from transformers import pipeline, set_seed, GPT2Tokenizer"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "hqj962lVA_OM", "outputId": "dd92f0cf-0f28-4c8e-94f3-c4bea79c4f82"}, "execution_count": 2, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"]}]}, {"cell_type": "code", "source": ["import os\n", "import nltk"], "metadata": {"id": "CwcjN5oFBAFi"}, "execution_count": 3, "outputs": []}, {"cell_type": "code", "source": ["file_path = \"unit1.txt\""], "metadata": {"id": "0UqOJLGtBAC7"}, "execution_count": 4, "outputs": []}, {"cell_type": "code", "source": ["try:\n", "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n", "        text = f.read()\n", "    print(\"File loaded successfully!\")\n", "except FileNotFoundError:\n", "    print(f\"Error: '{file_path}' not found.\")"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "A3X1lousBAAl", "outputId": "a658ea70-6577-4743-ba5a-6c5c4ff935de"}, "execution_count": 5, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["File loaded successfully!\n"]}]}, {"cell_type": "code", "source": ["print(\"--- Data Preview ---\")\n", "print(text[:500] + \"...\")"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "r0SFi7PYA_9v", "outputId": "48f873f3-3d91-4783-8474-eb43397a0482"}, "execution_count": 6, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["--- Data Preview ---\n", "Generative AI and Its Applications: A Foundational Briefing\n", "\n", "Executive Summary\n", "\n", "This document provides a comprehensive overview of Generative AI, synthesizing foundational concepts, technological underpinnings, and practical applications as outlined in the course materials from PES University. Generative AI represents a transformative subset of Artificial Intelligence focused on creating novel content, a capability primarily driven by the advent of Large Language Models (LLMs). The evolution of ...\n"]}]}, {"cell_type": "code", "source": ["set_seed(42)"], "metadata": {"id": "Xcg_JvXrA_3e"}, "execution_count": 7, "outputs": []}, {"cell_type": "code", "source": ["prompt = \"Generative AI is a revolutionary technology that\""], "metadata": {"id": "FCZ8AImnA_0Y"}, "execution_count": 8, "outputs": []}, {"cell_type": "code", "source": ["fast_generator = pipeline('text-generation', model='distilgpt2')\n", "\n", "# Generate text\n", "output_fast = fast_generator(prompt, max_length=50, num_return_sequences=1)\n", "print(output_fast[0]['generated_text'])"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 547, "referenced_widgets": ["2e8367144e6048c3b3c081691754431e", "d7241533c5504df1aad786019e69dbec", "938e56ab62314fe581220944e52133be", "0dbbb05cff1d4f6aa78af93d510e1a5b", "a5462de72503499fb48ea22c5539bb6e", "ec80eb5d0e784b47826a078481e6317a", "8b1a5a593d4c4c4f9ba0141c288daf29", "6312550da8984674b45d388e35e277c2", "51a0bfdf88a74f8a98745da4bbf6f655", "8586a0bb03c741f5ba120519a0eb87c7", "36551f02ef414ad2aaac49b645a10b49", "771a8f15ae084fec8fd834279dedef3e", "3b808f2bd3d74d0ea4fe20a87c25df2c", "a30ecb9a6c5a4027b76b5443f5e2b97e", "38df9caee71c43e2a2beb45818682c4b", "b4656dd43a8a4aab88030ab0e581928e", "16d95a23e640401fae54c721089d0769", "1e442012ae46468a9c4a337b4d2fa782", "54315123583f48d0bcbcbfb0d8ee2b7c", "c0aaf6130eeb429c80ce9b3d9b17d84a", "fb04ed8b639b4a869c26e9e6ee56cfd7", "06eccc638c334152a4de30e7bcf3b7b5", "c4bd7b33123947c3a5add9ab9920d959", "494ecfe361164302ada95ac8b8fe6440", "d141707b42b3400bbf971be1669c20cd", "7dd4c4f6e24c41dcace12c9152cdb6c2", "5d9b9f553d454493965ffd0a66cf8186", "0852260712a34529ace10ada8ffb54b2", "a25707243b84461ba3db70e1895eb45e", "a3258e110b024e548fcc077aa37dd2c4", "c7fe0e60498b4c538670d4655735db16", "9b704642abf64857a7f4fb695ca6abc2", "cdc76e49a8ea43439b9286e7554cdb7f", "082a9c6863dd45769b87ec2618f667b8", "620ece4c542a459ab1ad66776b4be36f", "de676b6f40e144ecae7a10780a65540e", "757512ab2215401c95f815fc6e38c6fc", "a6ac30d55c5e4b07b30e7aac9b15af08", "1f6a35ac645f48eb9120bbe3c50e5f88", "613483d6a1634acfb3cbe11566556742", "395ffd07e394432fb758880c6ac42edb", "3b519bbd1aca49abbe1834a55f00a4f5", "47a433313ac44f2badcea0948900d122", "316179310c7b4bf8bd02c00abf0cdfe6", "af8f2ca5bfda49dc98fc934e395e2485", "3ba966415a8f4eb7a71053ed214c16c4", "91c8603f76924888a87dd4b3ee2732f4", "a273eb915b614bfeafbae5aeaefb535d", "1388e98542c74962a52d9504b2dfd2c7", "ece8de673d464894b66222c838f59a60", "f0216421cba64a2dac697087ed38fe65", "43a29ae4a9fe447ebe12e2195ad1b37c", "4f12d120a73840e08d4fd4eb9c8780ba", "8ea5cee7f04c4d9da13151ae43db50f3", "9a1fc5e3f46f4bef9015faacfbc1988d", "259359457ecd470aa56ed63024454c9d", "358451821bd842ca828742cda8d0e7a0", "31216d5aaacc404486f1c9447abf31f3", "042d1220d57040b69d6d119252c951a6", "d8abc857ec3e4512921de3edcd4f6c9c", "c0112821db7249d8b204d94dd7c68223", "a84f20e30b6c4a9ea8225f9f560ad392", "e5d208acca8948e2b62a8ab95fa14dac", "b21ceeb44eb94fbf9ca85f09d58363f3", "413674380bd44cb291dc5165bca1edfd", "97475e466965491e8a282042aa3ff47f", "c3dcee9f51a54bac9badc0304406528b", "de3e5aa4751741b4a64d386b5573bb6e", "4d88583278d54ad4ba9867196ad4d46b", "78626d3b357440a3a1095f7bce560c5d", "a4ff09c2271d4788914aa2f71b61402a", "1c76570bf33c4616b925da4c4c13d867", "24285e4e6f42406c92d2e0a6f72f33be", "73761a184f714b578085a967f8e6db10", "f6d9472803d847c68f6521e8428e1dc2", "0c7d34d37789439fb5f3c6231a58e849", "08d7fc407a2242038bc3e5fe806489c8"]}, "id": "Zt8MRVSMBPEK", "outputId": "58540be5-f36d-4fa0-e99a-2042ce0940d9"}, "execution_count": 9, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n", "The secret `HF_TOKEN` does not exist in your Colab secrets.\n", "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n", "You will be able to reuse this secret in all of your notebooks.\n", "Please note that authentication is recommended but still optional to access public models or datasets.\n", "  warnings.warn(\n"]}, {"output_type": "display_data", "data": {"text/plain": ["config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2e8367144e6048c3b3c081691754431e"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "771a8f15ae084fec8fd834279dedef3e"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c4bd7b33123947c3a5add9ab9920d959"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "082a9c6863dd45769b87ec2618f667b8"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "af8f2ca5bfda49dc98fc934e395e2485"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "259359457ecd470aa56ed63024454c9d"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c3dcee9f51a54bac9badc0304406528b"}}, "metadata": {}}, {"output_type": "stream", "name": "stderr", "text": ["Device set to use cpu\n", "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n", "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n", "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]}, {"output_type": "stream", "name": "stdout", "text": ["Generative AI is a revolutionary technology that is designed to work with existing AI systems. It has been developed by the University of California, Berkeley. Its research team is the leading developer of AI software and its use is limited to AI and AI systems.\n", "\n", "\n", "The research team led by Professor Daniel Kranz, from the University of California, Berkeley, has developed a program to learn how to use the AI to improve the performance of the software. It has been developed by the University of California, Berkeley. Its research team is the leading developer of AI software and its use is limited to AI and AI systems. It is a top-selling research computer software company, and is a top-selling research computer software company.\n", "The research team developed the program to learn how to use the AI to improve the performance of the software. It has been developed by the University of California, Berkeley, Berkeley, and is a top-selling research computer software company. The research team developed the program to learn how to use the AI to improve the performance of the software. It has been developed by the University of California, Berkeley, and is a top-selling research computer software company.\n", "The research team developed the program to learn how to use the AI to improve the performance of the software. It has been developed by\n"]}]}, {"cell_type": "code", "source": ["smart_generator = pipeline('text-generation', model='gpt2')\n", "\n", "output_smart = smart_generator(prompt, max_length=50, num_return_sequences=1)\n", "print(output_smart[0]['generated_text'])"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 493, "referenced_widgets": ["e1f1a39ce91a403ebc8c9e3d91676a43", "4ef4c435b83442828f06d82a1c45b7d3", "0b2f442051f34c0eab41baf9aee052ab", "578274a6baf741c0b08519d5d37c7d7b", "370d163fa9d4452888bf03d9ea6b9b47", "4d019f09c17f4112baf094dcac31e992", "d4afd8e8d0c04526be0553e65d0a64ce", "a16c89e7e9264ba6949b6acabe1423a9", "da7ee34095d246d3bc027185435ff94f", "f636cd5991d240008f44f1570e0a2786", "31f284db605a439c91cd017f06322ffb", "a49ee93141e144768671e51aeafb9d10", "5400045e30984df592cfa3d65277cdd3", "3951e4c14e4f45bba5df18e2eb5a321b", "4196acc61c0a42bdadf593d3c913085d", "ed0e8f553b0349e4a444288cfd095927", "9671848318c04c4d9452f6ce196bf3f1", "6c9191f105114523a9997379544a2832", "df869f3ac77a4594b0d222702da2ccaf", "16cacada169d4a2980eccd09c0c4e78e", "d20a5ce91ee346638cb9025d8d6fef0f", "ca8ad116ba924f4ba77e45d943a84649", "01e5be26628b418490c8c1973b850f6a", "042826ebe8054a19bbd5d6eab3edadbc", "05d3771d4ddf484b9462abb7ed2c2790", "83c59b0649154095847acb0188afb774", "8b263791dc1b41ff90a04db555de562a", "7c86de84efba488fb085f1d773b81941", "07a645f838704460b96e25325957f93e", "3ea14bb761af4172b6237e61879de36e", "2557bdf75f524f84b449749af2f462ab", "40d0065c413f4815ba2eace2142da16f", "e898c84fbd4a4d719abe54e183abb644", "d7e4dcb268bf4d34aeef9df11fdaae7f", "0d640c62739341eb846737953d66ab8b", "5d2cefbfc825404b9355c594c32e4dfa", "a2b055979de74d6abfdcf9811bc91a28", "48b6b7b26993427889731eec02c6ba3e", "d9f5c13e66d54565ba2e69cf30ac49ea", "af24d14c0f6a47e487c846ac66f136b0", "f92d1e883570443d99402464fbcf201c", "4a906ff4ee91458ab127626cea148a6f", "23eb354f3dab4816965c7a7c94ce679f", "2d5657874af54b8f93c119ed83cd374b", "c17e7b4cfd4943f5af9af31f4fa84951", "83e80fb09e474b758329d0a77506e49c", "1254d361b248495da330944dc5b29f27", "865f160790b24f7980a19bc75b8e4178", "48e34d3b6a4b4086b0b96f6ac5cd887a", "40866f50ceca4c9996b3b563593f1e00", "9a95dbc4dc3546c48f09b07259580238", "fae7ee8c1f974b6e8bd50845a169b3dd", "815e37e239564377ab4d52438c676173", "6f6ed1047bb8457bbb0f7812bd70e1f2", "c986b766103842108acb9c2f89480385", "321a3b11cda24dcab34f962c32ddca42", "f5056e5d43af404cb9dde25cb5edd1cb", "e403ad3f40f24ac8ae3ca97a4d79198c", "2815388233a24da8bc1fa684376a6372", "8385aa4b934b46539db683a8722d41b8", "ca221bc34e78484a96fbe7029cd5702c", "912e83bfef2b4076872933e23dc6e39e", "9ecb3d35cee74f69b476d97a71da755d", "f465944c2d6d4cd2996e36bcf6522c48", "f32d8a20ab034fb08c636290eec36b7e", "14940e85ff6a48a189cf94421dde118b", "f17801fcfcaf41bc8dbfc408d43524a6", "05b8ee31ace146789dc6775cf6dae766", "87129f99794147c6aae9fc95f690acff", "fbfd8ba6c71547fda2a802dea61698f2", "3e16fc246ad04c97ad8e3447b7f0ca52", "60e6855632f247b7b571df74dc953e0d", "00cd94817dff4356aa3883eb104bd73b", "6a1b8dc02a1b4cd3baaa8745245869ed", "d4c17884389347c699b2fa4ae265dfca", "bebe5a9523f44bc6bee88b548e715356", "c1f852a10d034da0a7a96095bde29bf6"]}, "id": "iaAFY7EwBQ8O", "outputId": "ade235cc-d072-4b46-9ced-c3839917bf51"}, "execution_count": 10, "outputs": [{"output_type": "display_data", "data": {"text/plain": ["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e1f1a39ce91a403ebc8c9e3d91676a43"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "a49ee93141e144768671e51aeafb9d10"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "01e5be26628b418490c8c1973b850f6a"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "d7e4dcb268bf4d34aeef9df11fdaae7f"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c17e7b4cfd4943f5af9af31f4fa84951"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "321a3b11cda24dcab34f962c32ddca42"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f17801fcfcaf41bc8dbfc408d43524a6"}}, "metadata": {}}, {"output_type": "stream", "name": "stderr", "text": ["Device set to use cpu\n", "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n", "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n", "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]}, {"output_type": "stream", "name": "stdout", "text": ["Generative AI is a revolutionary technology that allows users to build AI that can help solve complex problems. It brings together hundreds of different approaches to solve problems, from solving complex problems in a laboratory to solving complex problems in a city. The technology allows users to build a computer that makes decisions based on user input, not on intuition.\n", "\n", "The AI is a model of human intelligence, and has many aspects that are similar to artificial intelligence. It can learn from humans, and it can adapt to the environment. It can learn by experimenting with new ways of thinking, and it can learn by learning from its own experience.\n", "\n", "It is the main driving force behind the new Artificial Intelligence, and the AI is very important to the success of AI. The new AI is designed to work out problems that need to be solved in a way that is easy to understand and solve, and that is flexible enough to be easily adaptable to different environments.\n", "\n", "The AI is designed to be scalable and adaptable to different environments. It can be used to solve complex problems without relying on humans. It can be used to build a solution that is very quickly scalable, scalable, inexpensive, and adaptable to different environments.\n", "\n", "The new AI is designed to work out problems that need to be solved in a way that is\n"]}]}, {"cell_type": "code", "source": ["tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"], "metadata": {"id": "yAZynkzeBTCl"}, "execution_count": 11, "outputs": []}, {"cell_type": "code", "source": ["sample_sentence = \"Transformers revolutionized NLP.\""], "metadata": {"id": "iag2CL5zBVLC"}, "execution_count": 12, "outputs": []}, {"cell_type": "code", "source": ["tokens = tokenizer.tokenize(sample_sentence)\n", "print(f\"Tokens: {tokens}\")"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "GmE3PtD8BWzd", "outputId": "650b0ae0-c84f-4779-a32f-debc0fb644b2"}, "execution_count": 13, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Tokens: ['Transform', 'ers', '\u0120revolution', 'ized', '\u0120N', 'LP', '.']\n"]}]}, {"cell_type": "code", "source": ["token_ids = tokenizer.convert_tokens_to_ids(tokens)\n", "print(f\"Token IDs: {token_ids}\")"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "8PrVCaF_BYno", "outputId": "088bcba1-f7fa-4e1e-bea3-b72354d46ce1"}, "execution_count": 14, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Token IDs: [41762, 364, 5854, 1143, 399, 19930, 13]\n"]}]}, {"cell_type": "code", "source": ["nltk.download('averaged_perceptron_tagger', quiet=True)\n", "nltk.download('punkt', quiet=True)\n", "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n", "nltk.download('punkt_tab', quiet=True)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "XZCjuE7tBajS", "outputId": "734ef7bd-97cf-4b73-d1a1-285b7c14277b"}, "execution_count": 15, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["True"]}, "metadata": {}, "execution_count": 15}]}, {"cell_type": "code", "source": ["pos_tags = nltk.pos_tag(nltk.word_tokenize(sample_sentence))\n", "print(f\"POS Tags: {pos_tags}\")"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "v2zd_aiyBczU", "outputId": "6e938035-1e6b-49c9-fbb2-65338ddfa07a"}, "execution_count": 16, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["POS Tags: [('Transformers', 'NNS'), ('revolutionized', 'VBD'), ('NLP', 'NNP'), ('.', '.')]\n"]}]}, {"cell_type": "code", "source": ["ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 236, "referenced_widgets": ["cde0c2d371c94383990d84e79d98e9f9", "006444d1a4bc45eb9457986e3a67f101", "e3c7a3bc8ce04e50b11c00e2b7a6eda0", "7f31c701ab37440996b8831375e572fa", "6a739a9b8546496da421a44c4a5fa8de", "14b450478f9c44fbbd6915f54c2c232c", "0df83c7bcf884fd5bb9916db56eeba9e", "f9522a2d131a4aaf9cd6232ff2410e00", "8e8743c964f84aabbffb3cd875079342", "49829cf0411d42088d2d8bfa5e31be24", "06b1ef4ee7894c2e9444cb64be2d9aa4", "5e5f3cee2aa64966a6ea7994de584e20", "21864a7099384f718d1fe895d8c0cc5e", "7cb6d1e4c17d47638078f01053a5a581", "4f6fea03008a4068a88bf685c09f56a9", "5c38674da578432cb35ca24b74760365", "b67544acda784049827c2a3dcfe109cf", "4280dfaf02d14b1782e838fa48ec65ff", "8a893cc6067448fda29d3406093f6f86", "a4f3572857684d3f823bed1866c213cc", "41e249822f8d4ea4a9c64925f4e41e6c", "e30a4ace7ead4f918d0f1a93c733543f", "c7dc77251ab64b6f8b5855a36df6fd43", "40a5c3cfae8149fca2c64366ca80e8b4", "3324e39fcf63422c98d4620fc76123eb", "9804d80e0fc449d8b8744ed333f13d30", "7a6ecc90a6e2458aa60ce852da6337df", "2329c2f9b71e4e52a2f5d779020a6276", "897c8781b6734523a8fb47ec3fdf7225", "48c00be233e44492869633415a56663d", "0e03727575cb407680b3ff41be3aa2fe", "5933dbd8f1074700be3e6dcfcfc7109d", "137291b1a6ad40d88d504bea314a2f8b", "f0f547ab3d6c463e96c1bc301325386a", "e3768c8d931042b8a12b217acc883f14", "86cb6566fc1947cd8a0afc96b24ef032", "1896cecf359c44a1aec7969bc79e654c", "dc834d13184040c78c45881e118633fd", "e03047fa98a14d37a56cc8e29b02ebf7", "79c650991a8f4646823d0bce1c4c9a02", "477b038ca22e4422b3814c8a5049dfe5", "f42b1253994f4156b3e385839629acfa", "fade69450149403ca1045ec21a683298", "37cb3995566a4806b7b1036c624d7aea"]}, "id": "YNaHOpX2BfAN", "outputId": "974c7202-c339-4c55-99b0-2904d7cf472a"}, "execution_count": 17, "outputs": [{"output_type": "display_data", "data": {"text/plain": ["config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "cde0c2d371c94383990d84e79d98e9f9"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5e5f3cee2aa64966a6ea7994de584e20"}}, "metadata": {}}, {"output_type": "stream", "name": "stderr", "text": ["Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n", "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n", "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c7dc77251ab64b6f8b5855a36df6fd43"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["vocab.txt: 0.00B [00:00, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f0f547ab3d6c463e96c1bc301325386a"}}, "metadata": {}}, {"output_type": "stream", "name": "stderr", "text": ["Device set to use cpu\n"]}]}, {"cell_type": "code", "source": ["snippet = text[:1000]\n", "entities = ner_pipeline(snippet)\n", "\n", "print(f\"{'Entity':<20} | {'Type':<10} | {'Score':<5}\")\n", "print(\"-\"*45)\n", "for entity in entities:\n", "    if entity['score'] > 0.90:\n", "        print(f\"{entity['word']:<20} | {entity['entity_group']:<10} | {entity['score']:.2f}\")"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "LvNg8OtdBhI5", "outputId": "5938d4fa-59d9-4a57-d4f9-37f8767f096d"}, "execution_count": 18, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Entity               | Type       | Score\n", "---------------------------------------------\n", "AI                   | MISC       | 0.98\n", "PES University       | ORG        | 0.99\n", "AI                   | MISC       | 0.98\n", "Large Language Models | MISC       | 0.91\n", "LLMs                 | MISC       | 0.90\n", "Transformer          | MISC       | 0.99\n"]}]}, {"cell_type": "code", "source": ["transformer_section = \"\"\"\n", "The introduction of the Transformer architecture in the 2017 paper \"Attention is all you need\" was a watershed moment in AI. It provided a more effective and scalable way to handle sequential data like text, replacing older, less efficient methods like recurrence (RNNs) and convolutions.\n", "The fundamental innovation of the Transformer is the attention mechanism. This component allows the model to weigh the importance of different words (tokens) in the input sequence when making a prediction. In essence, for each word it processes, the model can \"pay attention\" to all other words in the input, helping it understand context, resolve ambiguity, and handle long-range dependencies. This is crucial for tasks like translation, summarization, and question answering.\n", "The Transformer architecture consists of an encoder stack (to process the input) and a decoder stack (to generate the output), both of which heavily utilize multi-head attention and feed-forward networks.\n", "\"\"\""], "metadata": {"id": "5tX0FLBvBjD0"}, "execution_count": 19, "outputs": []}, {"cell_type": "code", "source": ["fast_sum = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n", "res_fast = fast_sum(transformer_section, max_length=60, min_length=30, do_sample=False)\n", "print(res_fast[0]['summary_text'])"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 265, "referenced_widgets": ["c26740bd6e5f4c558e1b6a52ce7dc2fe", "2d100c9f74024d2eaad68cea2766fb88", "52c71d3831024146b8b011cae847302d", "4b2fdcb816e948179f6c28bb61a4ca46", "b30ce7f233674c9db7ac31443c52fca8", "61859d3f793d4a2280e9d136d40a3694", "7a2489d7239a42cb989e46a63a8c037a", "576f9b84b91c478d898b3558b77b5735", "286870c728b745af98a71433b9c52e62", "20c59e82f591451b9283a2eaab983f8b", "e0d63f387614402287c68b4413dcb93b", "2fcc4b5f5ec843318475fdbebe3a4586", "f7e01db9b44a4ffc9ad40bff1cce5b6f", "22dec37ed7e5486893a3262d6de58755", "e0565fad37e443c09bb9eb22502cd9b1", "d20b8b7df3d1456f9681db775566054d", "1bb00d724cb94ad18d5b1da6f163e3db", "2e42362359a34e9687a1f88e48664677", "db25343d53604af697e87b83b15d5369", "2ced3ce6119741cfb506ffc7fefd2c0b", "f5e338d4b75542328b63346e69469a01", "36f8b6ef64e14042b0f7366e352696d2", "f745c69633664eb484eeebb9a1592287", "f4b21da03bec40ecb3b699f18751b96e", "645ad51f489e48d6a97e86be5c2901c1", "c52e5fd07f884d5997d36660d1cdb257", "187b0f43397647ab94cedff30da87c9e", "396b854fc29349379257f0624ad58ed4", "1a55663b6f9541eb876c79e2078ae58d", "2702ff3cf46045d6993a3184ea00374b", "8c75038f4919495098c95a5da5e7fcaa", "e89ad33ae91e41269f2e37dfb05f89f6", "753e08aacd28407db7b9e047d08e68a4", "1742d0ba1f62495daabaf7efb8b15ba7", "53a971b55f78475399d5f1aa1988e752", "cbb6ae56eec34668a8142088d4c50eae", "3afb5740094345c081db08e30527af7e", "8bd1c9f0352d46fabb2cad037950bb29", "87ec6ef412e74169aff7416c9c8402c5", "259449c5aa5748108a62be760d12a070", "382dcd72e31641438cdaffa4189183da", "556580bc555a459498f51165fc696ba0", "340a0929d91744a1a9af313ab3d25893", "82bba6d99285405e80987d18987b6eff", "d62b7cce3b784723ac06758d51adbc57", "26496950f7714265aca854f590d1a3c0", "15da08ec59ed4075b441588b8809861b", "79eed9f3626e4e66af2bbfcc5d9fc7a1", "fbf59f63c1264b2a80ff1749fccdff05", "47708a98222142a3828eb84886304b27", "5a95193c5de84ecc855b707fcc9d8656", "22aa8df4b8d2434f9892f57979dbc535", "6331f4dde0f94278aeac611f77f17e4f", "30a7ea30c9df41fda4a6991825bb2747", "ab1b887ab041496d8941c642c90522f2", "0db9462105194373a678f58698ac1c38", "eb1d77b885fb478f83c2d0cc242114f4", "782ee52b0996440782090bded201d370", "0f157e6f38be449ab7ba471e4a50ab36", "0a4c74000f0d474290294ccb7743d9e1", "78d8b2e6780b4040b91b27a76062253c", "5a2d2346378e4dbea9cc3dab9d9d86c1", "199d92dec98141c182f57069a1dddc3f", "0b91304bb88d42b588c8bcb8ba72f45a", "63b9f0d7319b4a6a84efedd5aefe7ed4", "d13592954a294928b0535d55019a74b3"]}, "id": "NuezkUydBlO6", "outputId": "af0be382-9beb-464e-80a3-d5417ba88ba5"}, "execution_count": 20, "outputs": [{"output_type": "display_data", "data": {"text/plain": ["config.json: 0.00B [00:00, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c26740bd6e5f4c558e1b6a52ce7dc2fe"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2fcc4b5f5ec843318475fdbebe3a4586"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f745c69633664eb484eeebb9a1592287"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "1742d0ba1f62495daabaf7efb8b15ba7"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["vocab.json: 0.00B [00:00, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "d62b7cce3b784723ac06758d51adbc57"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["merges.txt: 0.00B [00:00, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0db9462105194373a678f58698ac1c38"}}, "metadata": {}}, {"output_type": "stream", "name": "stderr", "text": ["Device set to use cpu\n"]}, {"output_type": "stream", "name": "stdout", "text": [" The introduction of the Transformer architecture in the 2017 paper \"Attention is all you need\" was a watershed moment in AI . It provided a more effective and scalable way to handle sequential data like text, replacing older, less efficient methods like recurrence (RNNs) and conv\n"]}]}, {"cell_type": "code", "source": ["smart_sum = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n", "res_smart = smart_sum(transformer_section, max_length=60, min_length=30, do_sample=False)\n", "print(res_smart[0]['summary_text'])"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 265, "referenced_widgets": ["4915a53fe42a40109514ce7aff4fa02f", "b823884e5c6f483ca64e45e3dc621ba9", "07d6468d8ace48eeb8800687196e2b80", "568d6d7f77554c8d917f1d98cb201ab3", "34bb28779f6e4f98bd8d176cc3ddb1b5", "c318c6a545914103946c5154dff9135d", "d31dc4c7a43d481ea8017a4f78f40fab", "13976f8b7a21463d9f4e453ff5d194da", "eda85e87d24c4c128c20aa4bf2176ac2", "882479b4fda24e638dadb12fc138b86c", "91a6bb3c43344d5a9e296c45adbf4775", "5af6b5a17dad44518ba465b6ffa4cf06", "df48b0e083e6472da62e185f7176be2f", "5ab5f977029941c19851d7cc7f3009f4", "6202cd50821c4ca9b7112b1593fae28d", "dd876cff2a1d44c2bea35f02320e79ec", "13270ed71e4b422ea037c7a3430baeca", "c76764ee03884e82bc69db4f94478578", "c223f251f358446cb611d129e5d50f27", "af1db6926dfb45818ec539b6ade62764", "9fd3067a0b544c5f8f728503be38573c", "ce9c26aba598417fa7632f5684c92848", "9aba9b45029f4b1f962629fd3861fe0f", "0b5f928892724255a5ece0eaec10f957", "2bde3c46991b48eab4c9fe12e23d32b9", "dbe046074f4946569b7aa4c110c5cc64", "aa57da26bc714172a7cddb16c4ec3735", "26ad02c64c8644b3a0e5431015302c20", "06abe02f1ee345dda12cc5afb07eee1a", "9ce33297f2194098840decad45d764f5", "81227a2e8e4e47d08e348298e88d79fe", "fc2f11e491084e67bba5928d0383ba1d", "7066e3e310b94565bce72ae3acc932a8", "d518c15d54ae4d6f8da2d72ee5fd0536", "9462b7a7de874aee9915594488626b6a", "d28ae1f35edd4960a5c3b7cedbababe2", "f2b10ada78384f9e9628272d1f02d896", "44ea3b342a2840c3b1e773c58d0440bb", "feedbab9fffa4397b85f077f79e3fd15", "a8f155a733f54560a98a2dd11f70a895", "5ffa0a18a0004d088716eac9cf547160", "0166bd649a564e56af33730c0032e384", "352c1961d93c41b68cafc6b39705aa76", "628f6512ff76483ebe545a0f332287cc", "0a0bf298326c4832b5c49a2181818f24", "1a4cb28bfe4540f08ff6f6ceb32b415a", "484268d88c2d4a538b0bc2859f1c276c", "57ab19a58c624dd3b347c0018c00aae2", "04d0ed64fc044edd9b6fafd72c8c524f", "514b319ebb9644159965229b31cde6de", "0f2dce8efd9b42b28e40a6405d075565", "4857ca4f9b5e49b58c1007ff196c6abb", "93e04f29c0e34120ab7a24f3a532fbea", "374449f69a1d49218c0a0eac508a676a", "fdd91f2e9157472bb2c646885c9e3f4c", "af2bce3090834e7ba360ecdb90f0535e", "4b26f663f38e424995225a73c36cb334", "58cc256ca8bf49e59caa344ff6bb79c9", "c17338969dab493aa90d44e55077910d", "4e8e02e614d546c8bc6755f2be3bd380", "dff738b2d7464b3ab8795f3efd5c9d22", "f7a78517962740e1b7fc53309a070934", "0bfeb370623f4b53a5c387a59256dff0", "17c73a356e15463c89e44e1eb51da95d", "9b099d1c6dd34f66bf5d9b4190b32129", "4dde2e1796a04d249fedb0917df48607"]}, "id": "hu6MIzmmBnC8", "outputId": "f2a1aab6-eaf7-488c-b6f5-17f7fd8e5f47"}, "execution_count": 21, "outputs": [{"output_type": "display_data", "data": {"text/plain": ["config.json: 0.00B [00:00, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "4915a53fe42a40109514ce7aff4fa02f"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5af6b5a17dad44518ba465b6ffa4cf06"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "9aba9b45029f4b1f962629fd3861fe0f"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["vocab.json: 0.00B [00:00, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "d518c15d54ae4d6f8da2d72ee5fd0536"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["merges.txt: 0.00B [00:00, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0a0bf298326c4832b5c49a2181818f24"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer.json: 0.00B [00:00, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "af2bce3090834e7ba360ecdb90f0535e"}}, "metadata": {}}, {"output_type": "stream", "name": "stderr", "text": ["Device set to use cpu\n"]}, {"output_type": "stream", "name": "stdout", "text": ["The introduction of the Transformer architecture in the 2017 paper \"Attention is all you need\" was a watershed moment in AI. It provided a more effective and scalable way to handle sequential data like text.\n"]}]}, {"cell_type": "code", "source": ["qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 195, "referenced_widgets": ["f9105a1a83a4499b9502bdee7e3d6b52", "f06319b46d6a463588091f0b535b3f24", "0846dbb89a164abd87fcc8eb9b351d24", "9ab984c859214525bcaa4ff29cd0151c", "03e10bd217744f56ae021a2d41540917", "82684bda2b1b41128aa39ceafefa306a", "8434ecbb806d4b9286f0154ff0e2b318", "65aba5723d494922899fd24cbdb8b0dd", "5a15491ed03447139d7eed5d2a752c50", "f490aa8a40204d95b245a019d845cf8d", "1ccb91152874446088c66c5567d7c6d2", "54899f883f4e46acac05353fbbf88176", "d18ac4b80afa4855aeb2d3bd580ba4ec", "2cc4ca59b0404ccdb475bdcdd22bca87", "2a7c042b54c644f591837e73c6f03c1f", "63b70941193d4347b5b93eae3901760e", "7fcf5a2d730140ceaa9561e9dc046204", "23819fb009dc4cf183493218c2ddd9eb", "e73c9359dfb647ee82ebc204e310b7b0", "b0df333f4e024c25ae5f7ba5f8c49890", "536910c0959543299bf039fc758925e5", "c361d77cdc0247799bfea8efc45b9660", "871ad96f7e9445adb3a087db564619cd", "27edfd7e2d1f4c728f5f68b3f4776da9", "2a7d4bed65724bd5a4080092969f08e0", "a80b8b222b604df8b051e1141c707f4b", "d5dd23ed6dc2453e9231aaf0c7cb6fbf", "280903db2ab443b6b0df808dc9f70d2a", "eda1aa6d17cf426294c059f4e708f297", "a92625be0fbe4962ac8ba91f0fbece88", "82bc30ac9a1343f086de8c8544e4687e", "7cb454d4e1b945ab8287f7ff33c9394f", "220e169a84cf49e6bdfa35075132c870", "7dd9a0f2c60b44a88e9c3430e8656fc3", "a635592afe2a41d4a32b9ef918577fa6", "726b3f04e515402fbc798ce1ddef1f2c", "b9572fa3f6324cc8a8266cdab9e35e23", "83694e8f794f4a0dbcd9f952c6cb4e0c", "95054de12c32466a982737366d99ecea", "3af8db4fd6e44a33994d1ca9c0453ba3", "fb3b5fda025d4b8399dd15e61b0799cc", "6e4bbbc9a1404db88f150f9488f932f2", "98b5c269e82640079a4793cfb07dba76", "d265f33404254db29e8896a23e424694", "aad9bfdd8c0e49cebc6e7593bfcd23cf", "02dd0fc1314742719bab8db5af31089d", "17657f8cfcb14ad5bcd64d17b0f6a07c", "97cd4aec77cb454b8dbc2cc065c2623d", "24fc7c392dc040508aae1632c13ba808", "2be2e85123154d52943b24dc39eda2a1", "f2bd50ad317d431f8f710d375978b7ca", "8e77430b3a4d4fdaa7ae78d544000b2b", "c666577c761c49a49f2cb8fa14723256", "e4861a5a223f4a5694276d70aa32e34f", "27fe22f835644d74a0636a09e2c61ba7"]}, "id": "zgKcFLjHBpUM", "outputId": "557dc234-c096-4f84-cf8b-ed8604219238"}, "execution_count": 22, "outputs": [{"output_type": "display_data", "data": {"text/plain": ["config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f9105a1a83a4499b9502bdee7e3d6b52"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "54899f883f4e46acac05353fbbf88176"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "871ad96f7e9445adb3a087db564619cd"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "7dd9a0f2c60b44a88e9c3430e8656fc3"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "aad9bfdd8c0e49cebc6e7593bfcd23cf"}}, "metadata": {}}, {"output_type": "stream", "name": "stderr", "text": ["Device set to use cpu\n"]}]}, {"cell_type": "code", "source": ["questions = [\n", "    \"What is the fundamental innovation of the Transformer?\",\n", "    \"What are the risks of using Generative AI?\"\n", "]\n", "\n", "for q in questions:\n", "    res = qa_pipeline(question=q, context=text[:5000])\n", "    print(f\"\\nQ: {q}\")\n", "    print(f\"A: {res['answer']}\")"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "ug1f6YvrBrUf", "outputId": "ce1db908-36e3-4e55-a192-e445b48a9b4c"}, "execution_count": 23, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["\n", "Q: What is the fundamental innovation of the Transformer?\n", "A: to identify hidden patterns, structures, and relationships within the data\n", "\n", "Q: What are the risks of using Generative AI?\n", "A: data privacy, intellectual property, and academic integrity\n"]}]}, {"cell_type": "code", "source": ["mask_filler = pipeline(\"fill-mask\", model=\"bert-base-uncased\")"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 268, "referenced_widgets": ["6281a7d86d764cbba8c3ceb05c82452f", "0b2b4861f7e4456881a508b1f522ebb1", "a656b3d151804ffcb78d0396b25ef90f", "7c8a810a342b4c99a995466cc1ff23cd", "57d1384a9e7c4b23a56e39c895a92836", "0c9b85cb359c4e45bf163396dfdb705e", "e3e2234047ae49f79c462c637a147ef9", "c1ed80dfec2b48eabe632c750ff7220d", "b41b40c5d9ed4604a4582f1987823b74", "900e5fd781c040b18fa35ecd9e940747", "40f099cb8f9444e2a7e17af76327ad94", "9ae154d186f143eab2e0f691c249dfa6", "3816ad2905714452a3cedb3f5261ec71", "73dbb9f974114e02ab9e966ce38097dc", "69176a97c2d74988adcb92fafc1e3289", "cc69f3b51a8544f3b70e4e5fd5a3941c", "6d9fb98d3dd14955bcf31f78ae2dfa26", "ee5c19f5c9434001b834eb3517a2e307", "9ff0f22dc3bd4167a23cccdd3f345992", "4dc61494f1e64ab591c8a50a6a0ecf9e", "7e2efc2355d54392a00ed647fda57349", "8ee07bd52b444c7d8451d93cc39552be", "870119ff67784ecca9cc336ce0442176", "180c2f49f9d9498294fe28ae492192f8", "9e7c8a6165e14691bf0caca45594bb8c", "55e22260c0544de49dffabfda2c3bbce", "a3f86685faf54c4f9d1bff17c7f036cb", "e186570738104fec8192db5edc20b83e", "c9c29510d31f40d6b8634e958f55c4f4", "2ab0ea5562cf442c92c010f7d7f20d22", "6605cd03a1704da3a4d67c815b572549", "7e28b3f85ae64d6594e2a3ba2302f847", "72d541e869d14ce1b9e5dbd981d64d3c", "4ddcc3d7666c489c9030a15e63295bb6", "d4625b882de54771a71001cc760cc8e8", "de0608e08d404fddabb8268ea45d4da9", "9ff75a9fd1fc4629b43d6f239a278f13", "311a2074eb32423a9a4b952652bb33a0", "933ee9e8ab3e4ac6af60b8b481549ceb", "7e1cef1a31824130970224e252dea482", "75de2a2e6c02465d8466c98936f55857", "980b05e1695845c7bb1fa8d6fbc9d809", "3cec5638ee1348c0b316c95ba25073a8", "417c3c9ae80047fda06aaba9bea42ed6", "005846bdf10c4c8490eec59495241e77", "bdeb3893239c472d91c65374506ed1be", "9b0a873de68f466e8c9909bf1deb1f39", "d8c67c0beb8a402db95ab31de58f40ce", "841750809579416689583e2e9e5474e4", "de059d8a37134cf8917e670d749762bc", "bd98ee774a0249699dd8f7c8fa605596", "df80d7beebc24fc38e72dc82bda2a7cb", "441b75a3e9184164a187f198739d8188", "ab9c69be863c4f01b126e713be74cece", "2e2781438b1e414982bec188d2c36253"]}, "id": "IhxZbr2MBtLe", "outputId": "a219ba00-8406-4664-8184-43ec53d5f447"}, "execution_count": 24, "outputs": [{"output_type": "display_data", "data": {"text/plain": ["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "6281a7d86d764cbba8c3ceb05c82452f"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "9ae154d186f143eab2e0f691c249dfa6"}}, "metadata": {}}, {"output_type": "stream", "name": "stderr", "text": ["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n", "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n", "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "870119ff67784ecca9cc336ce0442176"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "4ddcc3d7666c489c9030a15e63295bb6"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "005846bdf10c4c8490eec59495241e77"}}, "metadata": {}}, {"output_type": "stream", "name": "stderr", "text": ["Device set to use cpu\n"]}]}, {"cell_type": "code", "source": ["masked_sentence = \"The goal of Generative AI is to create new [MASK].\"\n", "preds = mask_filler(masked_sentence)\n", "\n", "for p in preds:\n", "    print(f\"{p['token_str']}: {p['score']:.2f}\")"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "GFu2mhIoBvGP", "outputId": "b29b11fa-df91-415e-8efa-2e512d96d19e"}, "execution_count": 25, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["applications: 0.06\n", "ideas: 0.05\n", "problems: 0.05\n", "systems: 0.04\n", "information: 0.03\n"]}]}]}